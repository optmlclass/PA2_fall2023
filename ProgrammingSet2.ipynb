{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp_nCL_eeFqv"
      },
      "source": [
        "**IMPORTANT:** You should run this using a GPU! In Colab, go to the \"runtime\" menu, choose \"change runtime type\", and select \"GPU\" from the hardware accelerator dropdown menu. Otherwise, this might take much longer. \n",
        "\n",
        "There are **5** places you need to write code in this notebook, for questions **1**, **2a** and **2b**, **3a** and **3b**. All parts are with 10 points, for a total of 50 points.\n",
        "\n",
        "To turn in this homework: download as .ipynb (File -> download as .ipynb). Make the filename YOURNAME_PA2.ipynb and send via email attachment to opt4mlclass+fall2023@gmail.com. with your name and PA2 in the subject line.\n",
        "\n",
        "Be aware: this assignment may require up to 2 hours of computation even when everything is implemented correctly.\n",
        "\n",
        "Please read the comments and explanation code carefully. In particular, you may want to study the provided implementation of SGD to get an idea of how the pytorch optimizer class works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tohFb7lQUdI7"
      },
      "outputs": [],
      "source": [
        "# Clone attention modeling and training code needed for question 3\n",
        "!git clone https://github.com/optmlclass/PA2_fall2023.git\n",
        "\n",
        "# get harry potter data (also for question 3)\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%204%20-%20The%20Goblet%20of%20Fire.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV3jEpEcUqOa"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import Optimizer\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from collections.abc import Iterable\n",
        "\n",
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")\n",
        "\n",
        "# make deterministic\n",
        "from PA2.sequenceutils import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "# imports for attention model\n",
        "# more imports\n",
        "from PA2.attentionmodel import GPT, GPTConfig\n",
        "from PA2.sequenceutils import sample, CharDataset\n",
        "from PA2.attentiontrainer import Trainer, TrainerConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jprhL760UwwY"
      },
      "outputs": [],
      "source": [
        "# Basic SGD implementation for reference.\n",
        "# for base class, see https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py\n",
        "# for official pytorch SGD implementation, see https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
        "\n",
        "class SGD(Optimizer):\n",
        "  def __init__(self, params, lr=1.0):\n",
        "    super(SGD, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    # The params argument can be a list of pytorch variables, or\n",
        "    # a list of dicts. If it is a list of dicts, each dict should have \n",
        "    # a key 'params' that is a list of pytorch variables,\n",
        "    # and optionally another key 'lr' that specifies the learning rate\n",
        "    # for those variables. If 'lr' is not provided, the default value\n",
        "    # is the single value provided as an argument after params to this\n",
        "    # constructor.\n",
        "    # If params is just a list of pytorch variables, it is the same\n",
        "    # as if params were actually a list containing a single dictionary\n",
        "    # whose 'params' key value is the list of variables.\n",
        "    # See examples in following code blocks for use of params.\n",
        "\n",
        "    # Set up an iteration counter.\n",
        "    # self.state[p] is a python dict for each parameter p\n",
        "    # that can be used to store various state useful in the optimization\n",
        "    # algorithm. In this case, we simply store the iteration count, although\n",
        "    # it is not used in this simple algorithm.\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        state = self.state[p]\n",
        "        state['step'] = 0\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      lr = group['lr']\n",
        "\n",
        "      # it is common practice to call the model parameters p in code.\n",
        "      # in class we follow more closely analytical conventions, in which the\n",
        "      # parameters are often called w for weights.\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        # Update the iteration counter (again, this is not actually used in this algorithm)\n",
        "        state = self.state[p]\n",
        "        step = state['step']\n",
        "        step += 1\n",
        "        \n",
        "        # Perform the SGD update. p.grad holds the gradient of the loss\n",
        "        # with respect to p.\n",
        "        p -= lr * p.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7GJhIPkXICX"
      },
      "outputs": [],
      "source": [
        "#Simple linear regression problem\n",
        "dimension = 10\n",
        "num_iter = 10000\n",
        "\n",
        "mean = torch.zeros(dimension)\n",
        "std = torch.ones(dimension)\n",
        "\n",
        "def loss_func(w_hat, b_hat, w_true, b_true):\n",
        "  # simple linear regression problem, although\n",
        "  # slightly non-standard loss. See pytorch docs\n",
        "  # for description of loss function.\n",
        "\n",
        "  # features\n",
        "  x = torch.normal(mean, std)\n",
        "\n",
        "  # true label is a linear function of features plus noise.\n",
        "  noise = np.random.normal(0.0, 0.01)\n",
        "  y_true = torch.dot(x, w_true) + b_true + noise\n",
        "\n",
        "  y_hat = torch.dot(x, w_hat) + b_hat\n",
        "\n",
        "  loss = torch.nn.SmoothL1Loss()\n",
        "  return loss(y_hat, y_true)\n",
        "\n",
        "\n",
        "# Set \"true\" parameter value to be a random normal vector with covariance 10*I\n",
        "w_true = 10*torch.normal(mean, std)\n",
        "\n",
        "# make true bias term quite large so that it is better\n",
        "# to have a high learning rate for the bias. This makes\n",
        "# it advantageous to use the params as a dict in the \n",
        "# following cell.\n",
        "b_true = torch.normal(torch.zeros(1), torch.ones(1))\n",
        "\n",
        "\n",
        "# declare variables that will actually be trained.\n",
        "# \"requires_grad\" tells pytorch that it may have to compute\n",
        "# gradients with respect to these variables so that it initialize the \n",
        "# relevant autograd data structures.\n",
        "w = torch.zeros(dimension, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = SGD([w, b], 0.010)\n",
        "\n",
        "losses = []\n",
        "\n",
        "for t in range(num_iter):\n",
        "\n",
        "  # \"zero_grad\" resets all variable.grad values to 0 and resets all\n",
        "  # intermediate data that might have been saved for a backwards pass.\n",
        "  # This is useful when variables need to be reused for many backward passes.\n",
        "  # Note that your PA1 autograd implementation did not need to have this\n",
        "  # functionality because the SGD you implemented just created  new\n",
        "  # variables every iteration.\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Compute the loss function - this is the forward pass\n",
        "  loss = loss_func(w, b, w_true, b_true)\n",
        "\n",
        "  # loss.backward has essentially the same functionality as the .backward\n",
        "  # function you implemented in PA1.\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(loss.item())\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo8zvSzpbxgJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "w = torch.zeros(dimension, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "param_groups = [{'params': [w], 'lr': 0.1}, {'params': [b], 'lr': 0.01}]\n",
        "optimizer = SGD(param_groups, 0.10)\n",
        "\n",
        "losses = []\n",
        "for t in range(num_iter):\n",
        "  optimizer.zero_grad()\n",
        "  loss = loss_func(w, b, w_true, b_true)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(loss.item())\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgaaUrMPXCt5"
      },
      "source": [
        "Remarks\n",
        "\n",
        "1. WHat are these \"`p`\" values representing?\n",
        "\n",
        "In class, we frequently consider a single parameter $w$ to describe the model we are training, so that $w$ would represent all the weights in a neural network for example. However, in practice usually models are specified in a more modular manner. For example, we could have a linear model with a bias as:\n",
        "$$\n",
        "\\hat y = \\langle A, x\\rangle + B\n",
        "$$\n",
        "where $A\\in \\mathbb{R}^d$ is a weight vector and $B\\in\\mathbb{R}$ is a bias. For theoretical analyses, we might consider this model as parametrized by a single vector $w\\in \\mathbb{R}^{d+1}$, but when programming it is far simpler to have separate variables for $A$ and $B$. Just as in your previous programming assignment, the `A.grad` and `B.grad` variables will then be populated with  the partial derivatives of the overall loss with respect to $A$ and $B$. Thus, to implement a gradient based optimizer, we need to loop over all variables that were involved in specifying the model. `p` is the variable used to step through this loop, so in the linear model with bias example, `p` would take on both `A` and `B` as values.\n",
        "\n",
        "2. What does `@torch.no_grad` do? \n",
        "\n",
        "This tells pytorch not to record gradient operations for anything that happens inside the function, so it will not be able to compute derivatives of these operations. However, this is important to enable because some operations (like performing an update) inherently should not be differentiated, and pytorch will thrown an error if you do not explicitly tell it that you do not want to differentiate through this operation.\n",
        "\n",
        "3. Why do we need `torch.enable_grad` when using the closure?\n",
        "\n",
        "Computing the closure (closure is essentially a fancy name for \"function\") is an alternative to providing the gradient to the optimization step. Instead, the loss $\\ell(w_t, z_t)$ is computed by the closure. However, the closure might need to compute some derivatives, so we turn on `torch.enable_grad`.\n",
        "\n",
        "## PART 1: Per-Variable AdaGrad\n",
        "\n",
        "#Question 1\n",
        "\n",
        "1. Fill out the class below to implement a version of SGD that sets the \"adaptive\" learning rate on a per-pytorch variable basis:\n",
        "$$\n",
        "\\eta_t[p] = \\frac{lr}{\\sqrt{G_t[p]}}\n",
        "$$\n",
        "$lr$ (for \"learning rate\") is provided as input to the algorithm, $t$ is the current iteration count, and\n",
        "$$\n",
        "G_t[p] = \\sum_{i=1}^t \\|\\nabla \\ell(w_i, z_i)[p]\\|^2\n",
        "$$\n",
        "where here $p$ indicates a pytorch variable object (which could be a high-dimensional vector), and $\\nabla \\ell(w_t, z_t)[p]$ indicates the partial derivative with respect to $p$ (i.e. the coordinates of the gradient corresponding to $p$).\n",
        "If you are familiar with the AdaGrad update, note that this is NOT the same because AdaGrad uses a *per-coordinate* learning rate, while we use a *per-variable* learning rate. For very large models in which memory is at a premium, this kind of change can reduce the memory required by the optimizer while still preserving some of the adaptive properties of AdaGrad.\n",
        "\n",
        "Read the `__init__` method carefully as it has changed slightly from the original `SGD` class above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BZwfQwgf-i7"
      },
      "outputs": [],
      "source": [
        "class SGD_adaptive(Optimizer):\n",
        "  def __init__(self, params, lr=1.0):\n",
        "    super(SGD_adaptive, self).__init__(params, {'lr': lr})\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "        \n",
        "    ####### YOUR CODE HERE ########\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVFwzCFUcewz"
      },
      "outputs": [],
      "source": [
        "w = torch.zeros(dimension, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "optimizer = SGD_adaptive([w, b], 1.0)\n",
        "losses = []\n",
        "for t in range(num_iter):\n",
        "  optimizer.zero_grad()\n",
        "  loss = loss_func(w, b, w_true, b_true)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(loss.item())\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owhhpRSrSCJr"
      },
      "source": [
        "See https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py for some similar example of how to set up the process below, from which much of this was copied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM_sfJdXgkUq"
      },
      "outputs": [],
      "source": [
        "## These commands create two pytorch \"DataLoader\" objects, one for training\n",
        "# data and one for testing data.\n",
        "# A DataLoader object is essentially a list of training/testing examples with\n",
        "# some extra code attached by pytorch that can send the data to GPUs.\n",
        "\n",
        "# The following three commands produce a torchvision dataset object for both\n",
        "# training and testing data on the CIFAR10 dataset, which is a dataset of images.\n",
        "# these objects are like lists of vectors (the \"transform\" specifies a function\n",
        "# that converts images to vectors).\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "CIFAR_train = torchvision.datasets.CIFAR10(root='CIFAR10/', train=True, download=True, transform=transform)\n",
        "CIFAR_test = torchvision.datasets.CIFAR10(root='CIFAR10/', train=False, transform=transform)\n",
        "\n",
        "\n",
        "# The next two lines produce the actual DataLoaders. The batch_size argument sets\n",
        "# the batch size, so that the DataLoader will appear like a list [a,b,c...] where\n",
        "# each entry of the list is a batch of batch_size examples. \n",
        "# Setting shuffle=True makes it so that when you start iterating over the examples\n",
        "# in the DataLoader, it will first shuffle the order of the list.\n",
        "trainloader = torch.utils.data.DataLoader(CIFAR_train, batch_size=16,\n",
        "                                          shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(CIFAR_test, batch_size=256,\n",
        "                                          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcoCaPmQhBra"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQsA0rWgQrUn"
      },
      "outputs": [],
      "source": [
        "## Please read the function signatures and doc string for \"train_resnet\" function.\n",
        "# You do not need to otherwise understand how it works to do this homework. ##\n",
        "\n",
        "def adjust_learning_rate(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def train_resnet(optimizer, train_loader, device, lrs, wd=0.01, model=None):\n",
        "  '''\n",
        "  Trains a resnet model on CIFAR10 data.\n",
        "\n",
        "  args:\n",
        "    optimizer: optimizer class that will be instantiated inside this function.\n",
        "    train_loader: pytorch DataLoader object that provides the training examples.\n",
        "    device: pytorch device to use (e.g. a GPU).\n",
        "    lrs: list of learning rates. The length of the list is the number of epochs\n",
        "      to train for. At the start of the ith epoch, the 'lr' parameter of the\n",
        "      optimizer will be set to lrs[i].\n",
        "    wd: l2 regularization constant.\n",
        "    model: can provide a preset pytorch model to train from a checkpoint. If \n",
        "      None, will instantiate a fresh resnet18 model.\n",
        "\n",
        "  returns:\n",
        "    the trained pytorch model object.\n",
        "  '''\n",
        "\n",
        "  if model is None:\n",
        "    model = torchvision.models.resnet18(pretrained=False)\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "  # define \\ell as the cross-entropy function.\n",
        "  # this function as implemented in pytorch actually combines both the softmax\n",
        "  # and the cross entropy function into one. As a result, it is\n",
        "  # is non-negative, smooth, Lipschitz, and convex in its argument, which\n",
        "  # are the predicted scores for various classes output by some model.\n",
        "  # Note that this does NOT necessarily imply that it has these properties\n",
        "  # with respect to the parameters of the model.\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = SGD_adaptive(model.parameters(), lrs[0])\n",
        "\n",
        "  num_epochs = len(lrs)\n",
        "\n",
        "  average_loss = 0.0\n",
        "\n",
        "  for epoch, lr in enumerate(lrs):#range(num_epochs):\n",
        "    # iterate over training set. One full pass over the training set is called\n",
        "    # an \"epoch\".\n",
        "    # i is index of example, data is the example (e.g. z_i).\n",
        "    # because shuffle=True when defining the trainloader,\n",
        "    # the training set is shuffled after every complete pass.\n",
        "\n",
        "    adjust_learning_rate(optimizer, lr)\n",
        "\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "\n",
        "    for it, data in pbar:\n",
        "      # unpack the example data: inputs is a batch of images,\n",
        "      # labels is a batch of labels.\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # pytorch will keep details from old gradients around in case you are going\n",
        "      # to differentiate something else. We therefore need to delete this old\n",
        "      # data before computing the loss so that we only have the gradient\n",
        "      # for this iteration.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      predicted_labels = model.forward(inputs)\n",
        "      loss = cross_entropy_loss(predicted_labels, labels)\n",
        "\n",
        "      weight_decay = wd * 0.5 * sum([torch.linalg.norm(p)**2 for p in model.parameters()])\n",
        "\n",
        "      regularized_loss = loss + weight_decay\n",
        "\n",
        "      # tell pytorch to compute some gradients\n",
        "      regularized_loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      \n",
        "\n",
        "      average_loss += (loss.item() - average_loss)/(it+1)\n",
        "\n",
        "      pbar.set_description(f\"epoch {epoch + 1} iter {it + 1}: train loss {average_loss:.5f}.\")\n",
        "\n",
        "    average_loss = 0.0\n",
        "\n",
        "  print('\\nFinished Training')\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZEClnnmfy7f"
      },
      "outputs": [],
      "source": [
        "## Train the model for 3 epochs using a learning rate of 0.1 in each epoch.\n",
        "## If this fails, you probably have a significant bug in your SGD code.\n",
        "## Each epoch should take < 5 minutes, for a total of at most 15 minutes.\n",
        "trained_model = train_resnet(SGD_adaptive, trainloader, device, lrs=[0.1, 0.1, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbHMj8JOm52O"
      },
      "outputs": [],
      "source": [
        "# These functions take input a trained model and return either the train or\n",
        "# test accuracy.\n",
        "def get_train_accuracy(trained_model):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    trainloader = torch.utils.data.DataLoader(CIFAR_train, batch_size=256, num_workers=2)\n",
        "    for data in trainloader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = trained_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  return 100 * correct / total\n",
        "\n",
        "def get_test_accuracy(trained_model):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    testloader = torch.utils.data.DataLoader(CIFAR_test, batch_size=256, num_workers=2)\n",
        "    for data in testloader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = trained_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFBWPlufSliV"
      },
      "outputs": [],
      "source": [
        "## Print the test accuracy. The test accuracy should be ~50% because the learning\n",
        "## rate of 0.1 used earlier is not a good learning rate.\n",
        "test_accuracy = get_test_accuracy(trained_model)\n",
        "print(\"test accuracy percentage: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj4DLWC1CIyG"
      },
      "source": [
        "#Question 2a\n",
        "\n",
        "Write a function that returns a DataLoader object (like the `trainloader` variable) for the training set that uses the provided argument as a batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjSH4rPceb5J"
      },
      "outputs": [],
      "source": [
        "def get_trainloader(batch_size):\n",
        "  ### YOUR CODE HERE ###\n",
        "\n",
        "  \n",
        "  return trainloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ccDYE29-WJ7"
      },
      "source": [
        "#Question 2b\n",
        "\n",
        "Now, for each batch size in `[16, 64, 256]`, populate the dictionary `trained_models` with a model trained using that batch size for three epochs using `train_resnet`, the `SGD_adaptive` algorithm. Then run the next cell, which will print the train and test accuracies. **Tune the learning rate schedules to get the best test performance at batch size 16. You should get close to or better than 70%.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMG7M0pyYY8M"
      },
      "outputs": [],
      "source": [
        "## With batch size 16, each epoch should take < 5 minutes (for a total of 15 minutes).\n",
        "## With batch size 64, < 2 minutes.\n",
        "## With batch size 256 < 1 minute.\n",
        "batch_size_list = [16, 64, 256]\n",
        "trained_models = {16: None, 64: None, 256: None}\n",
        "for batch_size in batch_size_list:\n",
        "  lrs = #[first epoch lr, second epoch lr, third epoch lr]\n",
        "  ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRcHDkyi-lUT"
      },
      "outputs": [],
      "source": [
        "for batch_size in batch_size_list:\n",
        "  test_accuracy = get_test_accuracy(trained_models[batch_size])\n",
        "  train_accuracy = get_train_accuracy(trained_models[batch_size])\n",
        "  print(\"[batch size %d] train accuracy: %d , test accuracy: %d\" % (batch_size, train_accuracy, test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7-22PKTRal_"
      },
      "source": [
        "## PART 2: AdamW and Norm-scaling\n",
        "\n",
        "The next part of this homework assignment investigates implementing some variants of Adam. We will be testing your optimizers on a simplified implementation of [GPT](https://github.com/openai/gpt-3) based on the [minGPT](https://github.com/karpathy/minGPT) repository by Andrej Karpathy. This is a model that takes as input a sequence of 128 characters from a text file and attempts to predict the next character. This can be used to generate novel text by starting with a seed text string, and then repeatedly using the model to generate another character.\n",
        "\n",
        "\n",
        "# Tips\n",
        "\n",
        "* You may decrease the number of training epochs while debugging, but please set it back to 20 and run again before submission.\n",
        "\n",
        "* Study the provided AdaGrad implementation closely, it introduces a few pytorch functions that may be useful. You should check the documentation for these functions to see what they do.\n",
        "\n",
        "* You may occasionally need to restart the runtime (runtime->restart runtime). Sometimes the GPUs don't release memory properly, and sometimes the progress bars get a little messed up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkPmrmHQPdtO"
      },
      "source": [
        "# QUESTION 3a\n",
        "\n",
        "Implement the [AdamW](https://openreview.net/pdf?id=Bkg6RiCqY7) update *without* using the debiasing terms. AdamW performs the following (per-coordinate) update:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\eta_t\\left(\\frac{\\hat m_t}{\\sqrt{\\hat v_t} +\\epsilon} + \\lambda w_t\\right)\n",
        "$$\n",
        "where $\\hat m_t$ and $\\hat v_t$ are generated the same way as in the standard [Adam](https://openreview.net/pdf?id=Bkg6RiCqY7) update, and $\\lambda$ is an extra \"weight decay\" parameter provided to the optimizer and $\\eta_t$ is the learning rate parameter provided to the optimizer by the user.\n",
        "\n",
        "Ordinarily, \"weight decay\" is another word for L2 regularization. That is, the loss is modified to:\n",
        "$$\n",
        "\\mathcal{L}(w) + \\frac{\\lambda}{2}\\|w\\|^2\n",
        "$$\n",
        "This means that we could implement weight decay by changing the gradient to $\\nabla \\mathcal{L}(w) + \\lambda w$. The idea behind AdamW is that the weight-decay term is in some sense \"well-understood\" and should not be included in the $v_t$ and $A_t$ statistics that are being used to understand the more mysterious loss surface $\\mathcal{L}(w)$. See the linked paper for more details and full pseudocode.\n",
        "\n",
        "In your implementation, you should use the raw $m_t$ and $v_t$ values without applying the ``debiasing'' scheme involving dividing by $1-\\beta^{t+1}$ discussed in the papers.\n",
        "\n",
        "# QUESTION 3b\n",
        "\n",
        "Upgrade your debias-free AdamW implementation to use the `use_norm_scaling` argument of the `__init__` method. When this argument is `True`, you should scale the learning rate by the norm of the weights *for the given pytorch variable*. That is, for each variable $p$ you will replace the learning rate $\\eta_t$ at time $t$ with $\\|p\\|\\eta_t$ in the update:\n",
        "$$\n",
        "w_{t+1}[i] = w_t[i] - \\|w_t\\|_2\\eta_t\\left(\\frac{m_t[i]}{\\sqrt{v_t[i]} +\\epsilon} + \\lambda w_t[i]\\right)\n",
        "$$\n",
        "When the `use_norm_scaling` argument is false, simply perform the update from question 1a.\n",
        "\n",
        "This learning rate heuristic is inspired by a similar proposal for use with normalized updates in the [LARS](https://arxiv.org/abs/1708.03888) optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TAFPkUfSS70"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AdamW_bias(Optimizer):\n",
        "  def __init__(self, params, lr=1.0, betas=(0.9,0.999), use_norm_scaling=False):\n",
        "    super(AdamW_bias, self).__init__(params, {'lr': lr, 'beta1': betas[0], 'beta2': betas[1], 'weight_decay': 0.0})\n",
        "\n",
        "\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    self.use_norm_scaling = use_norm_scaling\n",
        "\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    # in this class, and also usually in practice, closure will always be None.\n",
        "    loss = None\n",
        "    epsilon = 1e-8\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "      \n",
        "    ## YOUR CODE HERE ##\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXc_WnyVSOmI"
      },
      "outputs": [],
      "source": [
        "# the \"block size\" is the number of characters the model takes as input.\n",
        "# in this case, it can look at up to 128 characters when predicting the next\n",
        "# character.\n",
        "block_size = 128 # spatial extent of the model for its context\n",
        "\n",
        "# For our training set, we will use the text of the first four Harry Potter books.\n",
        "text = open(\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", 'rb').read()\n",
        "text += open('J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt', 'rb').read()\n",
        "text += open('J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt', 'rb').read()\n",
        "text += open('J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt', 'rb').read()\n",
        "\n",
        "# text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFTg91uRPthh"
      },
      "outputs": [],
      "source": [
        "# generate the configuration for the model. These parameters specify\n",
        "# the neural network architecture we will be using. It is not necessary\n",
        "# to understand this model architecture.\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=8, n_head=8, n_embd=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLKHtyUsPuEl"
      },
      "outputs": [],
      "source": [
        "# generate training configurations for each of the optimizers. We will be testing\n",
        "# adam (official pytorch implementation)\n",
        "# adamw (official pytorch implementation)\n",
        "# your optimizer both with and without the norm_scaling flag set.\n",
        "def adamw_bias_factory(params, lr, betas):\n",
        "  return AdamW_bias(params, lr, betas)\n",
        "\n",
        "def adamw_bias_norm_scaling_factory(params, lr, betas):\n",
        "  return AdamW_bias(params, lr, betas, use_norm_scaling=True)\n",
        "\n",
        "optimizers = {\n",
        "    'adam': torch.optim.Adam, \n",
        "    'adamw': torch.optim.AdamW, \n",
        "    'adamw_bias': adamw_bias_factory, \n",
        "    'adamw_bias_norm_scaling': adamw_bias_norm_scaling_factory\n",
        "  }\n",
        "\n",
        "training_configs = {}\n",
        "\n",
        "for name, opt in optimizers.items():\n",
        "# construct a training config: this sets the learning rate, batch size, number \n",
        "# of epochs ect for each optimizer. warmup_tokens and final_tokens are parameters\n",
        "# used to setup a warm-up and decay learning rate scheduler.\n",
        "  training_configs[name] = TrainerConfig(max_epochs=5, batch_size=256, learning_rate=6e-4, optimizer=opt,\n",
        "                        lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
        "                        num_workers=2)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gSApvJa_zte"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faU9I2fR_0-K"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxFY3AXTPvLf"
      },
      "outputs": [],
      "source": [
        "# train a model on each optimizer, keeping track of the best-performing one.\n",
        "# each epoch should take less than 1 minute to train, for a total of at most\n",
        "# 4 algorithms * 5 epochs / algorithm * 1 minutes / epoch = 20 minutes training time\n",
        "# More likely, each epoch will take closer to 30 seconds.\n",
        "losses = {}\n",
        "min_loss = float('inf')\n",
        "best_model = None\n",
        "best_optimizer = None\n",
        "for name, tconf in training_configs.items():\n",
        "  print(\"training new model with optimizer: {}\".format(name))\n",
        "  model = GPT(mconf)\n",
        "  trainer = Trainer(model, train_dataset, None, tconf)\n",
        "  train_loss = trainer.train()\n",
        "  losses[name] = train_loss\n",
        "  train_dataset = CharDataset(text, block_size)\n",
        "  print(\"final epoch train loss: {}\".format(train_loss))\n",
        "  if train_loss < min_loss:\n",
        "    best_optimizer = name\n",
        "    # min_loss = train_loss\n",
        "\n",
        "print(\"best optimizer: {} with loss: {}\".format(best_optimizer, min_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1pDN13SGxf9"
      },
      "outputs": [],
      "source": [
        "## Now that we have identified a best optimizer, let us train for a longer amount of time (another 40 minutes):\n",
        "long_train_conf = TrainerConfig(max_epochs=20, batch_size=256, learning_rate=6e-4, optimizer=optimizers[best_optimizer],\n",
        "                        lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
        "                        num_workers=2)\n",
        "print(\"training new model with optimizer: {}\".format(name))\n",
        "best_model = GPT(mconf)\n",
        "train_dataset = CharDataset(text, block_size)\n",
        "trainer = Trainer(best_model, train_dataset, None, long_train_conf)\n",
        "train_loss = trainer.train()\n",
        "print(\"final epoch train loss: {}\".format(train_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE48jgJLPwvT"
      },
      "outputs": [],
      "source": [
        "# Now we will generate text at the character level, starting from the seed\n",
        "# \"Harry opened the door, and\"\n",
        "# Note, this model is not advanced enough to learn how to actualy write sentences\n",
        "# that makes sense, but it CAN learn to write complete words and certain names.\n",
        "# With longer training times, bigger models, and more data, it is possible to\n",
        "# achieve truly amazing results!\n",
        "\n",
        "context = [ord(c) for c in \"Harry opened the door, and\"]\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(best_model, x, 2000, temperature=0.9, sample=True, top_k=5)[0]\n",
        "completion = ''.join([chr(train_dataset.itos[int(i)]) for i in y])\n",
        "print(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGwWZ1Ppy0SW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EC500_Spring2022_ProgrammingSet2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
